{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELOS\n",
    "\n",
    "## FEATURES\n",
    "\n",
    "precipitation,temp_max,temp_min, wind, humidity, pressure, solar_radiation, visibility & coludiness_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTACION DE LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, classification_report, confusion_matrix, roc_auc_score, accuracy_score\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Librerias de modelos\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstatespace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarimax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SARIMAX\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBRegressor\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprophet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prophet\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "# Bibliotecas estándar\n",
    "import os\n",
    "import joblib\n",
    "import concurrent.futures\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Bibliotecas científicas\n",
    "import numpy as np\n",
    "\n",
    "# Bibliotecas de análisis y modelado de datos\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "\n",
    "# Librerias de modelos\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from xgboost import XGBRegressor\n",
    "from prophet import Prophet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Bibliotecas de visualización\n",
    "from plotly import graph_objs as go\n",
    "from prophet.plot import plot_plotly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'.\\Datos_Proyecto\\observations_full.csv')\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df.set_index('date',inplace=True)\n",
    "\n",
    "df = df.asfreq('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de variables para las que queremos hacer predicciones\n",
    "variables = ['precipitation', 'temp_max', 'temp_min', 'wind', 'humidity', 'pressure', 'solar_radiation', 'visibility', 'weather_id', 'cloudiness_id']\n",
    "\n",
    "# Ruta donde se guardarán los modelos entrenados\n",
    "models_path = \"sarima_models\"\n",
    "os.makedirs(models_path, exist_ok=True)  # Crear el directorio si no existe\n",
    "\n",
    "# Función para ajustar y guardar el modelo si no existe, o cargarlo si ya está guardado\n",
    "def fit_or_load_model(variable_name):\n",
    "    model_filename = os.path.join(models_path, f\"{variable_name}_sarima.pkl\")\n",
    "    \n",
    "    if os.path.exists(model_filename):\n",
    "        # Si el modelo ya existe, cargarlo\n",
    "        sarima_model = joblib.load(model_filename)\n",
    "        print(f\"Modelo SARIMA para {variable_name} cargado desde {model_filename}\")\n",
    "    else:\n",
    "        # Si el modelo no existe, ajustarlo y guardarlo\n",
    "        model = SARIMAX(df[variable_name],\n",
    "                        order=(1, 1, 1),\n",
    "                        seasonal_order=(1, 1, 1, 12),\n",
    "                        enforce_stationarity=False,\n",
    "                        enforce_invertibility=False)\n",
    "        \n",
    "        sarima_model = model.fit(disp=False, method='powell')\n",
    "        joblib.dump(sarima_model, model_filename, compress=4)\n",
    "        print(f\"Modelo SARIMA para {variable_name} ajustado y guardado en {model_filename}\")\n",
    "    \n",
    "    return sarima_model\n",
    "\n",
    "# Función para hacer predicciones usando un modelo ajustado o cargado\n",
    "def forecast_with_model(variable_name):\n",
    "    sarima_model = fit_or_load_model(variable_name)\n",
    "    forecast = sarima_model.get_forecast(steps=31)\n",
    "    forecast_mean = forecast.predicted_mean\n",
    "    return forecast_mean\n",
    "\n",
    "# Ejecutar las predicciones en paralelo usando ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    forecasts = list(executor.map(forecast_with_model, variables))\n",
    "\n",
    "# Almacenar los resultados en un DataFrame para facilidad de visualización\n",
    "predictions_df = pd.DataFrame({variables[i]: forecasts[i] for i in range(len(variables))})\n",
    "\n",
    "predictions_df.to_csv(r'C:\\Users\\joant\\OneDrive\\Stucom\\MasterIA\\BigData\\Projecte3_Meteorologia\\Entregables\\Code\\predictions.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time-based features\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "\n",
    "# Drop the original date column\n",
    "df = df.drop(columns=['date'])\n",
    "\n",
    "# Select features and targets\n",
    "features = ['precipitation', 'temp_max','temp_min', 'wind', 'humidity', 'pressure', \n",
    "            'solar_radiation', 'visibility', 'weather_id', 'estacion_id', 'cloudiness_id', \n",
    "            'year', 'month', 'day']\n",
    "\n",
    "targets = ['precipitation', 'temp_max','temp_min', 'wind', 'humidity', 'pressure']\n",
    "\n",
    "for target in targets:\n",
    "\n",
    "    features_clean = [feature for feature in features if feature != target]\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df[features_clean]\n",
    "    y = df[target]\n",
    "\n",
    "    # Split data into train and test sets (using time-based split for time series)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Train a Random Forest model for prediction\n",
    "    model = RandomForestRegressor(n_estimators=300, max_depth=15, min_samples_split=5, min_samples_leaf=4, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\nModelo {target}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "    # Save the model using joblib\n",
    "    model_path = fr'D:\\STUCOM\\Master_IABD\\Projecte3_Meteorologia\\Entregables\\Code\\SVM\\features_prediction\\{target}_rfr_model.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    print(f\"Modelo {target} guardado en: {model_path} (intro)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROPHET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\STUCOM\\Master_IABD\\Projecte3_Meteorologia\\Datos_Proyecto\\observations_full.csv')\n",
    "\n",
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(x=df['date'], y=df['humidity'], name='humidity',line_color='red'))\n",
    "# fig.layout.update(title_text='Time Series data with Rangeslider',xaxis_rangeslider_visible=True)\n",
    "# fig.show()\n",
    "\n",
    "targets = ['precipitation','temp_max','temp_min','wind','humidity','pressure','solar_radiation','visibility','cloudiness_id']\n",
    "\n",
    "for target in targets:\n",
    "\n",
    "    X = df[['date',target]]\n",
    "    y = df[target]\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    train_df['ds'] = pd.to_datetime(X['date'])\n",
    "    train_df['y']=y\n",
    "    train_df.head(2)\n",
    "\n",
    "    model = Prophet()\n",
    "    model.fit(train_df)\n",
    "    joblib.dump(model,fr\"D:\\STUCOM\\Master_IABD\\Projecte3_Meteorologia\\Entregables\\Code\\Prophet\\models\\prophet_{target}.pkl\")\n",
    "\n",
    "\n",
    "    future = model.make_future_dataframe(periods=365)\n",
    "    forecast = model.predict(future)\n",
    "    forecast.to_csv(fr\"D:\\STUCOM\\Master_IABD\\Projecte3_Meteorologia\\Entregables\\Code\\Prophet\\Predictions_2025\\predictions_{target}.csv\")\n",
    "\n",
    "    # fig1 = plot_plotly(model, forecast)\n",
    "    # fig1.show()\n",
    "\n",
    "    # #plot component wise forecast\n",
    "    # fig2 = model.plot_components(forecast)\n",
    "    # fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# juntar las predicciones de todos los targets en un solo data frame\n",
    "df_predictions = pd.DataFrame()\n",
    "\n",
    "path = r'D:\\STUCOM\\Master_IABD\\Projecte3_Meteorologia\\Entregables\\Code\\Prophet\\Predictions_2025'\n",
    "\n",
    "_, _, files = next(os.walk(path))\n",
    "    \n",
    "for file in files:\n",
    "\n",
    "    df = pd.read_csv(os.path.join(path,file))\n",
    "\n",
    "    if 'cloudiness' in file:\n",
    "        df['yhat'] = df['yhat'].round(0).astype(int)\n",
    "\n",
    "    \n",
    "    df_predictions[re.search(r'predictions_(.*?).csv', file).group(1)] = df['yhat']\n",
    "\n",
    "df_predictions.insert(0,'date',df['ds'])\n",
    "\n",
    "df_predictions = df_predictions.drop(df_predictions.index[:25000]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_predictions.to_csv(r'D:\\STUCOM\\Master_IABD\\Projecte3_Meteorologia\\Datos_Proyecto\\predictions\\predictions_Prophet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(r\"D:\\STUCOM\\Master_IABD\\Projecte3_Meteorologia\\Datos_Proyecto\\observations_full.csv\")\n",
    "\n",
    "# Extract time-based features\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "\n",
    "# Drop the original date column\n",
    "df = df.drop(columns=['date'])\n",
    "\n",
    "# Select features and targets\n",
    "features = ['year', 'month', 'day', 'estacion_id']\n",
    "X = df[features]\n",
    "\n",
    "targets = ['precipitation', 'temp_max','temp_min', 'wind', 'humidity', 'pressure','solar_radiation','visibility','cloudiness_id']\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 1000],\n",
    "    'max_depth': [10, 20, 50],\n",
    "    'min_child_weight': [5, 8],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.7, 0.8],\n",
    "    'colsample_bytree': [0.9, 1.0]\n",
    "}\n",
    "\n",
    "for target in targets:\n",
    "    # Separate features and target\n",
    "    y = df[target]\n",
    "\n",
    "    # Split data into train and test sets (using time-based split for time series)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Initialize XGBoost model\n",
    "    model_xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "    # Perform GridSearchCV to find the best hyperparameters\n",
    "    grid_search = GridSearchCV(estimator=model_xgb, param_grid=param_grid, cv=3, scoring='neg_mean_absolute_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "    # Fit the grid search model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameters from the grid search\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"\\nMejores parámetros para el modelo {target}: {best_params}\")\n",
    "\n",
    "    # Refit the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Evaluate the best model on the test data\n",
    "    y_pred_xgb = best_model.predict(X_test)\n",
    "    mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "    rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "\n",
    "    print(f\"\\nModelo {target} - XGBoost (Con mejores parámetros)\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae_xgb}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse_xgb}\")\n",
    "\n",
    "    # Save the best XGBoost model\n",
    "    model_xgb_path = fr'D:\\STUCOM\\Master_IABD\\Projecte3_Meteorologia\\Entregables\\Code\\XGBoost\\{target}_xgb_model_best.pkl'\n",
    "    joblib.dump(best_model, model_xgb_path)\n",
    "    print(f\"Modelo {target} guardado en: {model_xgb_path} (XGB - Mejores parámetros)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEATHER_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "features = ['year', 'month', 'day', 'precipitation', 'temp_max', 'temp_min', 'wind', 'humidity', 'pressure', 'solar_radiation', 'visibility', 'cloudiness_id']\n",
    "target = 'weather_id'\n",
    "\n",
    "# Split data into train and test sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# model = SVC(kernel='rbf', C=0.1, gamma='auto', class_weight='balanced', probability=True) Model 3\n",
    "model = SVC(kernel='rbf', C=0.1, gamma='auto', probability=True) # Model 4\n",
    "\n",
    "\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ROC AUC (for multi-class)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test_scaled), multi_class='ovr')\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "\n",
    "# Save the model using joblib\n",
    "joblib.dump(model, r'D:\\STUCOM\\Master_IABD\\Projecte3_Meteorologia\\Entregables\\Code\\SVM\\weather_id_svm_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "features = ['year', 'month', 'day', 'precipitation', 'temp_max', 'temp_min', 'wind', 'humidity', 'pressure', 'solar_radiation', 'visibility', 'cloudiness_id']\n",
    "target = 'weather_id'\n",
    "\n",
    "# Split data into train and test sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# RANDOM FOREST\n",
    "\n",
    "model_rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "model_rf.fit(X_train_res, y_train_res)\n",
    "y_pred_rf = model_rf.predict(X_test_scaled)\n",
    "\n",
    "print(\"Random Forest Classifier - Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "joblib.dump(model_rf,r'D:\\STUCOM\\Master_IABD\\Projecte3_Meteorologia\\Entregables\\Code\\RandomForest\\weather_id_RF.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICCIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las predicciones usare el modelo que mejores resultados ha dado respecto a sus predicciones.\n",
    "\n",
    "Mejor modelo para features: **XGBoosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joant\\AppData\\Local\\Temp\\ipykernel_1732\\142018936.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictions_XGBoosting = pd.concat([predictions_XGBoosting, row_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to .\\..\\weather_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Path to the models\n",
    "path = r'.\\..\\Modelos\\XGBoost'\n",
    "modelos_xgb = os.listdir(path)\n",
    "\n",
    "# Date range for predictions\n",
    "initial_date = '2024-12-01'\n",
    "final_date = '2025-12-31'\n",
    "\n",
    "# Create a DataFrame for the date range\n",
    "dates = pd.date_range(start=initial_date, end=final_date)\n",
    "dates_df = pd.DataFrame({\n",
    "    'year': dates.year,\n",
    "    'month': dates.month,\n",
    "    'day': dates.day\n",
    "})\n",
    "\n",
    "thresholds = {\n",
    "    'precipitation':[13.94,1,-13.94],\n",
    "    'temp_max':[9.21,1,-9.21],\n",
    "    'temp_min':[5.62,1,-5.62],\n",
    "    'wind':[2.24,1,-2.24],\n",
    "    'humidity':[18.00,1,-18.00],\n",
    "    'pressure':[17.67,1,-17.67],\n",
    "    'solar_radiation':[305.49,1,-305.49],\n",
    "    'visibility':[5.07,1,-5.07],\n",
    "    'cloudiness_id':[0.52,1,-0.52]\n",
    "}\n",
    "\n",
    "# Initialize an empty DataFrame to store predictions\n",
    "predictions_XGBoosting = pd.DataFrame(columns=[\n",
    "    'date', 'precipitation', 'temp_max', 'temp_min', 'wind', 'humidity',\n",
    "    'pressure', 'solar_radiation', 'visibility', 'cloudiness_id'\n",
    "])\n",
    "\n",
    "# Loop through each date\n",
    "for _, row in dates_df.iterrows():\n",
    "\n",
    "    year, month, day = row['year'], row['month'], row['day']\n",
    "\n",
    "    data = {\n",
    "        'date': datetime.strptime(f'{year}-{month}-{day}', \"%Y-%m-%d\"),\n",
    "        'precipitation': None,\n",
    "        'temp_max': None,\n",
    "        'temp_min': None,\n",
    "        'wind': None,\n",
    "        'humidity': None,\n",
    "        'pressure': None,\n",
    "        'solar_radiation': None,\n",
    "        'visibility': None,\n",
    "        'cloudiness_id': None\n",
    "    }\n",
    "\n",
    "    # Loop through each model\n",
    "    for modelo_xgb in modelos_xgb:\n",
    "\n",
    "        # Load the model\n",
    "        modelo = joblib.load(os.path.join(path, modelo_xgb))\n",
    "        \n",
    "        # Extract target name from the model filename\n",
    "        target_name = modelo_xgb.replace(\"_xgb_model_best.pkl\", \"\")\n",
    "\n",
    "        # Create a 2D array with the input features\n",
    "        feature_row = pd.DataFrame([[year, month, day]], columns=['year', 'month', 'day'])\n",
    "        \n",
    "        # Predict using the model\n",
    "        prediccion = (modelo.predict(feature_row)[0]) - thresholds[target_name][random.randint(0,2)]\n",
    "\n",
    "        if \"cloudiness\" in modelos_xgb:\n",
    "            # Update the corresponding field in the data dictionary\n",
    "            data[target_name] = prediccion.round(0).astype(int)\n",
    "        else:\n",
    "            data[target_name] = prediccion\n",
    "\n",
    "    # Append the data dictionary to the DataFrame\n",
    "    row_df = pd.DataFrame([data])\n",
    "    predictions_XGBoosting = pd.concat([predictions_XGBoosting, row_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "output_path = r'.\\..\\weather_predictions.csv'\n",
    "predictions_XGBoosting.to_csv(output_path, index=False)\n",
    "print(f\"Predictions saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejor modelo para weather_id: **Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joant\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\joant\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.5.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\joant\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cargar el DataFrame y convertir la columna de fecha\n",
    "predictions = pd.read_csv(r'.\\..\\weather_predictions.csv')\n",
    "predictions['date'] = pd.to_datetime(predictions['date'])\n",
    "\n",
    "# Extraer año, mes y día de la fecha\n",
    "predictions['year'] = predictions['date'].dt.year\n",
    "predictions['month'] = predictions['date'].dt.month\n",
    "predictions['day'] = predictions['date'].dt.day\n",
    "\n",
    "# Cargar el modelo\n",
    "modelo_RFC = joblib.load(r'.\\..\\Modelos\\RandomForest\\Classifier\\weather_id_RFC.pkl')\n",
    "\n",
    "# Seleccionar las características requeridas para el modelo\n",
    "features = [\n",
    "    'year', 'month', 'day', 'precipitation', 'temp_max', 'temp_min',\n",
    "    'wind', 'humidity', 'pressure', 'solar_radiation', 'visibility', 'cloudiness_id'\n",
    "]\n",
    "\n",
    "# Crear la matriz de características para el modelo\n",
    "X = predictions[features]\n",
    "\n",
    "# Generar predicciones con el modelo\n",
    "predictions['weather_id'] = modelo_RFC.predict(X).round(0).astype(int)\n",
    "\n",
    "# Guardar el DataFrame actualizado\n",
    "predictions.to_csv(r'.\\..\\weather_predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
